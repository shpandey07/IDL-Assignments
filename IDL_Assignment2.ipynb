{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IDL_Assignment2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PsG3qDDaFN_Z","colab_type":"text"},"source":["**GROUP MEMBERS:**\n","\n","Asad Karim -- 226063 \n","\n","\n","Shweta Pandey -- 225964 \n","\n","\n","Uzain Jabbar -- 220658 \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7hU8oJwdxtO_","colab_type":"text"},"source":["# # Assignment 2"]},{"cell_type":"markdown","metadata":{"id":"n-yHelf1w3z0","colab_type":"text"},"source":["## An MLP training script using tf.data"]},{"cell_type":"code","metadata":{"id":"T8pyDguz3Rlj","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3guDJQnvpTi","colab_type":"code","outputId":"574438de-815b-475f-cc03-4b8b861229fd","colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["mnist = tf.keras.datasets.mnist\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","\n","# Iterating through the data\n","for img, lbl in train_data:\n","    print(lbl.numpy())\n","    plt.imshow(img.numpy(), cmap=\"Greys_r\")\n","    plt.show()\n","    input()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"J-OAVGLgd4dM","colab_type":"code","outputId":"6ab8b028-1c80-474f-bfa5-5cf71ad01ea3","executionInfo":{"status":"error","timestamp":1588665939512,"user_tz":-330,"elapsed":1167,"user":{"displayName":"Shweta Pandey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKmPwAuapBlfsEI0yrIRM0-OJG1eOl6-Bigdrixu8=s64","userId":"16619661468802363035"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["train_data = tf.data.Dataset.range(100)\n","train_data = train_data.shuffle(1000)\n","train_data = train_data.repeat()\n","train_data = train_data.batch(20)\n","\n","\n","iterator   = train_data.make_one_shot_iterator()\n","next_batch = iterator.get_next()\n","\n","with tf.compat.v1.Session() as sess:\n","    for i in range(15):\n","        if i % (10//2) == 0:\n","            print(\"------------\")\n","        print(sess.run(\"{:02d}:\".format(i), next_batch.eval()))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ad56f8bf23af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0miterator\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'make_one_shot_iterator'"]}]},{"cell_type":"markdown","metadata":{"id":"F-n2z80_eay4","colab_type":"text"},"source":["If we change the order of the above three operations,we will get different results. If the repeat operation is applied before shuffle opeartion,certain elements/images will be repeated before other elements appear even once. Also, all samples might not be processed in one epoch.\n","If the shuffle operation is applied before the repeat operation, then performance might slow down at the beginning of each epoch.\n","\n","Best Order:\n","\n","For batches to be different in each epoch, we should shuffle first, then repeat, and finally batch because all batches are unique."]},{"cell_type":"code","metadata":{"id":"7j2HdXEOJ41Q","colab_type":"code","colab":{}},"source":["# normalize the images to [0, 1]  and reshape images from (28, 28) to (784,) and convert labels to int32\n","\n","train_images = (train_images.astype(np.float32) / 255.).reshape((-1, 784))\n","test_images = (test_images.astype(np.float32) / 255.).reshape((-1, 784))\n","\n","train_labels = train_labels.astype(np.int32)\n","test_labels = test_labels.astype(np.int32)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","train_data = train_data.batch(128)\n","\n","# print the shapes\n","\n","for img_batch, lbl_batch in train_data:\n","    print(img_batch.shape, lbl_batch.shape)\n","    \n","   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-lOYzQpvv4k","colab_type":"code","colab":{}},"source":["train_steps = 1000\n","learning_rate = 0.1\n","\n","n_input = 784\n","h1 = 512\n","h2 = 128\n","n_classes = 10\n","\n","# weights and bias initializations\n","W1 = tf.Variable(tf.random.uniform(shape = (n_input,h1), minval = -(math.sqrt(6)/math.sqrt(n_input+h1)),  \n","                            maxval = (math.sqrt(6)/math.sqrt(n_input+h1)))) # Xavier uniform\n","W2 = tf.Variable(tf.random.uniform(shape = (h1,h2), minval = -(math.sqrt(6)/math.sqrt(h1+h2)),\n","                             maxval = (math.sqrt(6)/math.sqrt(h1+h2)))) \n","out = tf.Variable(tf.random.uniform(shape = (h2,n_classes), minval = -(math.sqrt(6/(h2+n_classes))),\n","                                   maxval = math.sqrt(6/(h2+n_classes)) ))\n","\n","b1 = tf.Variable(tf.random.uniform([h1]))\n","b2 = tf.Variable(tf.random.uniform([h2]))\n","b_out = tf.Variable(tf.random.uniform([n_classes]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LLnAUvRqDDa_","colab":{}},"source":["\n","for step in range(train_steps):\n","    img_batch, lbl_batch = data.next_batch()\n","   \n","    with tf.GradientTape() as tape:\n","        logit1 = tf.nn.relu(tf.matmul(img_batch, W1) + b1)\n","        logit2 = tf.nn.relu(tf.matmul(logit1, W2) + b2)\n","        output = tf.matmul(logit2,out) + b_out\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=output, labels=lbl_batch))\n","    grads = tape.gradient(xent, [W1, b1, W2, b2, out, b_out])    \n","    \n","    W1.assign_sub(learning_rate * grads[0])\n","    b1.assign_sub(learning_rate * grads[1])\n","    W2.assign_sub(learning_rate * grads[2])\n","    b2.assign_sub(learning_rate * grads[3])\n","    out.assign_sub(learning_rate * grads[4])\n","    b_out.assign_sub(learning_rate * grads[5])\n","\n","        \n","    if not step % 100:\n","        preds = tf.argmax(output, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n","                             tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"apjqXh8vDLA9","colab":{}},"source":["test_preds1 = tf.matmul(data.test_data, W1) + b1\n","test_preds2 = tf.matmul(test_preds1, W2) + b2\n","test_preds3 = tf.argmax(tf.matmul(test_preds2, out) + b_out, axis=1,\n","                       output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds3, data.test_labels),\n","                             tf.float32))\n","print(acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PjaLD3HoFha","colab_type":"text"},"source":["## Fail 1\n"]},{"cell_type":"code","metadata":{"id":"mKcPTc47n6dm","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","from datasets import MNISTDataset\n","from time import time\n","import os\n","\n","# get the data\n","(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n","mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n","                     test_imgs.reshape((-1, 784)), test_lbls,\n","                     batch_size=256, seed=int(time()))\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 8\n","w_range = 0.4\n","\n","# just set up a \"chain\" of hidden layers\n","layers = []\n","for layer in range(n_layers):\n","    layers.append(tf.keras.layers.Dense(\n","        n_units, activation=tf.nn.relu,\n","        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n","                                                         maxval=w_range),\n","        bias_initializer=tf.initializers.constant(0.001)))\n","\n","\n","# Adding tf.board writer \n","import time\n","logdir = os.path.join(\"logs\", \"fail_one\" + str(time.time()))\n","fail1_train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n","fail1_test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n","\n","\n","# finally add the output layer\n","layers.append(tf.keras.layers.Dense(\n","    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n","                                                         maxval=w_range)))\n","\n","lr = 0.1\n","for step in range(2000):\n","    img_batch, lbl_batch = mnist.next_batch()\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        out = img_batch\n","        for layer in layers:\n","            out = layer(out)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=out, labels=lbl_batch))\n","\n","    weights = [var for l in layers for var in l.trainable_variables]\n","    grads = tape.gradient(xent, weights)\n","    for grad, var in zip(grads, weights):\n","        var.assign_sub(lr*grad)\n","\n","    # Change 2\n","    with fail1_train_writer.as_default():\n","      tf.summary.scalar(\"loss\", xent, step=step)\n","      tf.summary.histogram(\"logits\", out, step=step)\n","      tf.summary.histogram(\"weights\", var, step=step)\n","\n","\n","\n","    if not step % 100:\n","        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","        with fail1_train_writer.as_default():\n","            tf.summary.scalar(\"accuracy\", acc, step=step)\n","            tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n","\n","\n","out = mnist.test_data\n","for layer in layers:\n","    out = layer(out)\n","test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(test_acc))\n","\n","with fail1_test_writer.as_default():\n","            tf.summary.scalar(\"accuracy\", test_acc, step=step)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnIGmXccpoZV","colab_type":"code","colab":{}},"source":["# then load/run tensorboard\n","\n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6Axtcj1WeUg","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOEXeZUtsDPq","colab_type":"text"},"source":["## Fail 2"]},{"cell_type":"code","metadata":{"id":"bNhA63WWoKdl","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","from datasets import MNISTDataset\n","from time import time\n","\n","\n","# get the data\n","(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n","mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n","                     test_imgs.reshape((-1, 784)), test_lbls,\n","                     batch_size=256, seed=int(time()))\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 8\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","layers = []\n","for layer in range(n_layers):\n","    layers.append(tf.keras.layers.Dense(\n","        n_units, activation=tf.nn.sigmoid,\n","        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n","                                                         maxval=w_range),\n","        bias_initializer=tf.initializers.constant(0.001)))\n","    \n","# Adding tf.board writer \n","import time\n","logdir = os.path.join(\"logs\", \"fail_two\" + str(time.time()))\n","fail2_train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n","fail2_test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n","\n","# finally add the output layer\n","layers.append(tf.keras.layers.Dense(\n","    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n","                                                         maxval=w_range)))\n","\n","lr = 0.1\n","for step in range(2000):\n","    img_batch, lbl_batch = mnist.next_batch()\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        out = img_batch\n","        for layer in layers:\n","            out = layer(out)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=out, labels=lbl_batch))\n","\n","    weights = [var for l in layers for var in l.trainable_variables]\n","    grads = tape.gradient(xent, weights)\n","    for grad, var in zip(grads, weights):\n","        var.assign_sub(lr*grad)\n","    # Change 2\n","    with fail2_train_writer.as_default():\n","      tf.summary.scalar(\"loss\", xent, step=step)\n","      tf.summary.histogram(\"logits\", out, step=step)\n","      tf.summary.histogram(\"weights\", var, step=step)\n","\n","    if not step % 100:\n","        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","        with fail2_train_writer.as_default():\n","          tf.summary.scalar(\"accuracy\", acc, step=step)\n","          tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n","\n","\n","out = mnist.test_data\n","for layer in layers:\n","    out = layer(out)\n","test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(test_acc))\n","\n","with fail2_test_writer.as_default():\n","            tf.summary.scalar(\"accuracy\", test_acc, step=step)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIGlUdAGsLKz","colab_type":"code","colab":{}},"source":["# then load/run tensorboard\n","\n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SK876A7sPhg","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYFHXcwJsTuS","colab_type":"text"},"source":["## Fail 3"]},{"cell_type":"code","metadata":{"id":"fgXAGpUHsSK-","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","from datasets import MNISTDataset\n","from time import time\n","\n","\n","# get the data\n","(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n","mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n","                     test_imgs.reshape((-1, 784)), test_lbls,\n","                     batch_size=256, seed=int(time()))\n","\n","\n","# define the model first, from input to output\n","\n","# let's use fewer layers...\n","n_units = 100\n","n_layers = 2\n","\n","# just set up a \"chain\" of hidden layers\n","layers = []\n","for layer in range(n_layers):\n","    layers.append(tf.keras.layers.Dense(\n","        n_units, activation=tf.nn.relu,\n","        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n","                                                         maxval=0.),\n","        bias_initializer=tf.initializers.constant(0.001)))\n","    \n","\n","# Adding tf.board writer \n","import time\n","logdir = os.path.join(\"logs\", \"fail_three\" + str(time.time()))\n","fail3_train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n","fail3_test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n","\n","\n","# finally add the output layer\n","layers.append(tf.keras.layers.Dense(\n","    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n","                                                         maxval=0.01)))\n","\n","lr = 0.1\n","for step in range(2000):\n","    img_batch, lbl_batch = mnist.next_batch()\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        out = img_batch\n","        for layer in layers:\n","            out = layer(out)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=out, labels=lbl_batch))\n","\n","    weights = [var for l in layers for var in l.trainable_variables]\n","    grads = tape.gradient(xent, weights)\n","    for grad, var in zip(grads, weights):\n","        var.assign_sub(lr*grad)\n","\n","    # Change 2\n","    with fail3_train_writer.as_default():\n","      tf.summary.scalar(\"loss\", xent, step=step)\n","      tf.summary.histogram(\"logits\", out, step=step)\n","      tf.summary.histogram(\"weights\", var, step=step)\n","\n","\n","\n","    if not step % 100:\n","        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","        with fail3_train_writer.as_default():\n","          tf.summary.scalar(\"accuracy\", acc, step=step)\n","          tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n","\n","\n","out = mnist.test_data\n","for layer in layers:\n","    out = layer(out)\n","test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(test_acc))\n","\n","with fail2_test_writer.as_default():\n","            tf.summary.scalar(\"accuracy\", test_acc, step=step)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cweqkzwcsa06","colab_type":"code","colab":{}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nX_0TyIPsdqb","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yZ2KVQ1sgmD","colab_type":"text"},"source":["## Fail 4"]},{"cell_type":"code","metadata":{"id":"2WeEMBXCsfS_","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","from datasets import MNISTDataset\n","from time import time\n","\n","\n","# get the data\n","(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n","mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n","                     test_imgs.reshape((-1, 784)), test_lbls,\n","                     batch_size=256, seed=int(time()))\n","\n","\n","# define the model first, from input to output\n","\n","# let's use fewer layers...\n","n_units = 100\n","n_layers = 2\n","\n","# just set up a \"chain\" of hidden layers\n","layers = []\n","for layer in range(n_layers):\n","    layers.append(tf.keras.layers.Dense(\n","        n_units, activation=tf.nn.relu,\n","        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n","                                                         maxval=0.01),\n","        bias_initializer=tf.initializers.constant(0.001)))\n","\n","# Adding tf.board writer \n","import time\n","logdir = os.path.join(\"logs\", \"fail_four\" + str(time.time()))\n","fail4_train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n","fail4_test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n","\n","\n","# finally add the output layer\n","layers.append(tf.keras.layers.Dense(\n","    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n","                                                         maxval=0.01)))\n","\n","lr = 0.1\n","for step in range(2000):\n","    img_batch, lbl_batch = mnist.next_batch()\n","    # I hear adding random noise to inputs helps with generalization!\n","    img_batch = img_batch + tf.random.normal(tf.shape(img_batch), stddev=4)\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        out = img_batch\n","        for layer in layers:\n","            out = layer(out)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=out, labels=lbl_batch))\n","\n","    weights = [var for l in layers for var in l.trainable_variables]\n","    grads = tape.gradient(xent, weights)\n","    for grad, var in zip(grads, weights):\n","        var.assign_sub(lr*grad)\n","# Change 2\n","    with fail4_train_writer.as_default():\n","      tf.summary.scalar(\"loss\", xent, step=step)\n","      tf.summary.histogram(\"logits\", out, step=step)\n","      tf.summary.histogram(\"weights\", var, step=step)\n","\n","    if not step % 100:\n","        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","        with fail2_train_writer.as_default():\n","          tf.summary.scalar(\"accuracy\", acc, step=step)\n","          tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n","\n","\n","\n","out = mnist.test_data\n","for layer in layers:\n","    out = layer(out)\n","test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8tCE8fa0smAz","colab_type":"code","colab":{}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5AS7fZ-soUC","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJJr7K1KsrbK","colab_type":"text"},"source":["## Fail 5"]},{"cell_type":"code","metadata":{"id":"JV-iwqfpsqGL","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","from datasets import MNISTDataset\n","from time import time\n","\n","\n","# get the data\n","(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n","mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n","                     test_imgs.reshape((-1, 784)), test_lbls,\n","                     batch_size=256, seed=int(time()))\n","\n","\n","# define the model first, from input to output\n","\n","# let's use fewer layers...\n","n_units = 100\n","n_layers = 2\n","\n","# just set up a \"chain\" of hidden layers\n","layers = []\n","for layer in range(n_layers):\n","    layers.append(tf.keras.layers.Dense(\n","        n_units, activation=tf.nn.relu,\n","        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n","                                                         maxval=0.01),\n","        bias_initializer=tf.initializers.constant(0.001)))\n","# Adding tf.board writer \n","import time\n","logdir = os.path.join(\"logs\", \"fail_five\" + str(time.time()))\n","fail5_train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n","fail5_test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n","\n","# finally add the softmax output layer :))\n","layers.append(tf.keras.layers.Dense(\n","    10, activation=tf.nn.softmax,\n","    kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n","                                                     maxval=0.01)))\n","\n","lr = 0.1\n","for step in range(2000):\n","    img_batch, lbl_batch = mnist.next_batch()\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        out = img_batch\n","        for layer in layers:\n","            out = layer(out)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=out, labels=lbl_batch))\n","\n","    weights = [var for l in layers for var in l.trainable_variables]\n","    grads = tape.gradient(xent, weights)\n","    for grad, var in zip(grads, weights):\n","        var.assign_sub(lr*grad)\n","# Change 2\n","    with fail5_train_writer.as_default():\n","      tf.summary.scalar(\"loss\", xent, step=step)\n","      tf.summary.histogram(\"logits\", out, step=step)\n","      tf.summary.histogram(\"weights\", var, step=step)\n","\n","\n","    if not step % 100:\n","        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","        with fail5_train_writer.as_default():\n","          tf.summary.scalar(\"accuracy\", acc, step=step)\n","          tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n","\n","\n","\n","out = mnist.test_data\n","for layer in layers:\n","    out = layer(out)\n","test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n","test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(test_acc))\n","\n","with fail5_test_writer.as_default():\n","            tf.summary.scalar(\"accuracy\", test_acc, step=step)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5AisSh7swd3","colab_type":"code","colab":{}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8g77ENhsy89","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-3_dH5xs1Ha","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}