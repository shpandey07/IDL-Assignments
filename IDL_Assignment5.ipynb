{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDLAssignment5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kehL3GOLkKF7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "GROUP MEMBERS:\n",
        "\n",
        "Asad Karim -- 226063\n",
        "\n",
        "Shweta Pandey -- 225964"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGn57edTw3ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqO5xfD_mUwC",
        "colab_type": "code",
        "outputId": "2bf0ae02-19cf-4e85-f09b-f14e1080bd5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN3VB0zfi4ic",
        "colab_type": "code",
        "outputId": "3284aee3-3ba4-4d76-f030-4d51eaba06be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python prepare_data.py shakespeare.txt skp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-26 06:38:13.786392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 5604 sequences...\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teHVeS90wy0A",
        "colab_type": "code",
        "outputId": "2fdcc903-704c-4d2b-8274-e976bc2aa3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from prepare_data import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x, 200))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 1, 'F': 2, 'I': 3, '\\n': 4, 'u': 5, 'q': 6, 'x': 7, 'Q': 8, 'b': 9, 'P': 10, 't': 11, 'o': 12, 'a': 13, 'V': 14, ';': 15, 'E': 16, 'D': 17, '-': 18, '.': 19, 'r': 20, 'p': 21, 'l': 22, 'R': 23, 'N': 24, 'm': 25, 'T': 26, '!': 27, 'g': 28, 'C': 29, 'Y': 30, 'Z': 31, \"'\": 32, '&': 33, 'z': 34, ',': 35, 'L': 36, '$': 37, '3': 38, 'K': 39, 'j': 40, 'O': 41, 'M': 42, 'y': 43, 'w': 44, 'n': 45, 'J': 46, 'W': 47, 'i': 48, 'A': 49, '?': 50, 's': 51, 'f': 52, 'k': 53, 'S': 54, 'H': 55, 'G': 56, 'v': 57, 'h': 58, ':': 59, 'U': 60, 'e': 61, 'B': 62, 'd': 63, 'X': 64, 'c': 65, '<S>': 0}\n",
            "66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D8hjQXmxS2p",
        "colab_type": "code",
        "outputId": "60def961-31a8-4d72-bf87-bbf54f5dc1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "tf.one_hot(next(iter(data.batch(128))),vocab_size)[127][199]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(66,), dtype=float32, numpy=\n",
              "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjTF2C1exU-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = vocab_size\n",
        "\n",
        "hidden_dim = 512\n",
        "\n",
        "output_dim = vocab_size\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "range = 0.2\n",
        "\n",
        "W_ih = tf.Variable(tf.random.uniform([input_dim, hidden_dim], minval = -range, maxval = range), dtype=np.float32)\n",
        "\n",
        "W_hh = tf.Variable(tf.random.uniform([hidden_dim, hidden_dim], minval = -range, maxval = range), dtype=np.float32)\n",
        "\n",
        "b_h = tf.Variable(tf.random.uniform([hidden_dim], minval = -range, maxval = range), dtype=np.float32)\n",
        "\n",
        "W_ho = tf.Variable(tf.random.uniform([hidden_dim, output_dim], minval = -range, maxval = range), dtype=np.float32)\n",
        "\n",
        "b_o = tf.Variable(tf.random.uniform([output_dim], minval = -range, maxval = range), dtype=np.float32)\n",
        "\n",
        "h_n = tf.reshape(tf.Variable(np.zeros(hidden_dim), dtype= np.float32),(1,512))\n",
        "\n",
        "def rnn(input, h_n):\n",
        "  input = tf.reshape(input,(1,66))\n",
        "  a = tf.matmul(input, W_ih) + b_h + tf.matmul(h_n, W_hh)\n",
        "  h_n = tf.reshape(tf.nn.sigmoid(a), (1,512))\n",
        "  o = b_o + tf.matmul(h_n, W_ho)\n",
        "  return h_n, o\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IxvagyVe_BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvHCut7UJsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = tf.Variable(20)\n",
        "n_time_steps = tf.Variable(128)\n",
        "seq_size = tf.Variable(200)\n",
        "e = tf.Variable(1)\n",
        "def train():\n",
        "  for epoch in tf.range(epochs):\n",
        "    #print(epoch)\n",
        "    print('Start of epoch %d', (epoch,))\n",
        "\n",
        "    for time_step in tf.range(n_time_steps):\n",
        "      #print(time_step)\n",
        "      char_n = tf.one_hot(next(iter(data.batch(128))),vocab_size)\n",
        "      with tf.GradientTape() as tape:\n",
        "        hidden_states = tf.TensorArray(tf.float32,size=0,dynamic_size=True, element_shape = (1,hidden_dim))\n",
        "        outputs = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        loss = tf.TensorArray(tf.float32,size=0,dynamic_size=True)\n",
        "        for t in tf.range(seq_size-1):\n",
        "          if tf.less(t, 1):\n",
        "            h_n = tf.reshape(tf.Variable(np.zeros(hidden_dim), dtype= np.float32),(1,512))\n",
        "          else:\n",
        "            h_n = hidden_states.read(t-1)\n",
        "          char_n_tmp = char_n[time_step][t]\n",
        "          hidden_state, output = rnn(char_n_tmp, h_n)\n",
        "          outputs.write(outputs.size(),output).mark_used()\n",
        "          hidden_states.write(t, hidden_state).mark_used()\n",
        "          #print(\"output\", output)\n",
        "          #print(\"labels\", char_n[time_step][t+1])\n",
        "          loss.write(t, tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=char_n[time_step][t+1]))).mark_used()\n",
        "        # time_ste.mark_used()\n",
        "        loss_value = loss.stack()\n",
        "        grads = tape.gradient(loss_value,[W_hh, W_ho, W_ih, b_h, b_o])\n",
        "        #print(zip(grads, [W_hh, W_ho, W_ih, b_h, b_o]))\n",
        "        optimizer.apply_gradients(zip(grads, [W_hh, W_ho, W_ih, b_h, b_o]))\n",
        "\n",
        "\n",
        "      if time_step % 200 == 0:\n",
        "        print('Training loss (for one batch) at step %s: %s' % (time_step, float(loss_value[-1])))\n",
        "        print('Seen so far: %s samples' % ((time_step + 1) * 128))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyn0M5txPjtM",
        "colab_type": "code",
        "outputId": "913ac17f-2145-4a13-d61c-412cb6ac4d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=0>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.6403665542602539\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=1>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.3942781686782837\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=2>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.39575669169425964\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=3>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.4170176088809967\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=4>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.4283335506916046\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=5>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.415494829416275\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=6>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.41581231355667114\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=7>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.4076482355594635\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=8>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.39374929666519165\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=9>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.34666502475738525\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=10>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.2970624566078186\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=11>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.2765972912311554\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=12>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.25808078050613403\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=13>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.24347446858882904\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=14>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.23234297335147858\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=15>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.22147150337696075\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=16>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.22072650492191315\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=17>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.21478188037872314\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=18>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.21878023445606232\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n",
            "Start of epoch %d (<tf.Tensor: shape=(), dtype=int32, numpy=19>,)\n",
            "Training loss (for one batch) at step tf.Tensor(0, shape=(), dtype=int32): 0.2222827821969986\n",
            "Seen so far: tf.Tensor(128, shape=(), dtype=int32) samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awgfCwdamHLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_extended(input, h_n):\n",
        "  input = tf.reshape(input,(1,66))\n",
        "  a = tf.matmul(input, W_ih) + b_h + tf.matmul(h_n, W_hh)\n",
        "  h_n = tf.reshape(tf.nn.sigmoid(a), (1,512))\n",
        "  o = tf.nn.softmax(b_o + tf.matmul(h_n, W_ho))\n",
        "  return h_n, o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMhM3v4bXsgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_nn = tf.convert_to_tensor(\n",
        "    ([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
        "h_nn = tf.reshape(tf.Variable(np.zeros(hidden_dim), dtype= np.float32),(1,512))\n",
        "\n",
        "def generate_text(char, h): \n",
        "  hidden, out = rnn_extended(char,h)\n",
        "  out = tf.reshape(out, (66))\n",
        "  index = tf.math.argmax(out, axis=0)\n",
        "  out = tf.one_hot(index, vocab_size)\n",
        "  #print(int(index))\n",
        "  return ind_to_ch.get(int(index)), hidden, out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5-qdFQCgsU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "177ef00d-4575-4ad7-c8a6-a4149738666a"
      },
      "source": [
        "txt_len = 1000\n",
        "words = ''\n",
        "for t in tf.range(txt_len):\n",
        "  if t == 0:\n",
        "    txt, hidden, out = generate_text(char_nn,h_nn)\n",
        "  else: \n",
        "    txt, hidden, out = generate_text(out, hidden)\n",
        "  words = words + txt\n",
        "\n",
        "\n",
        "print(words)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " the sender the senst the sers and the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s ard the s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NHjP3T5jSQj",
        "colab_type": "text"
      },
      "source": [
        "The text generated looks like its stuck in a loop. A local minima\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k8_4P45X6OQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}